{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business understanding\n",
    "------------\n",
    "\n",
    "This initial phase focuses on understanding the project objectives and requirements from a business perspective, then converting this knowledge into a data mining problem definition and a preliminary plan designed to achieve the objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine business objectives\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "The first objective of the data analyst is to thoroughly understand, from a business perspective, what the client really wants to accomplish. Often the client has many competing objectives and constraints that must be properly balanced. The analyst's goal is to uncover important factors, at the beginning, that can influence the outcome of the project. A possible consequence of neglecting this step is to expend a great deal of effort producing the right answers to the wrong questions.\n",
    "\n",
    "### output\n",
    "\n",
    "#### background\n",
    "\n",
    "Record the information that is known about the organization's business situation at the beginning of the project.\n",
    "\n",
    "#### business objectives \n",
    "\n",
    "Describe the customer's primary objective, from a business perspective. In addition to the primary business objective, there are typically other related business questions that the customer would like to address. For example, the primary business goal might be to keep current customers by predicting when they are prone to move to a competitor. Examples of related business questions are \"How does the primary channel (e.g., ATM, visit branch, internet) a bank customer uses affect whether they stay or go?\" or \"Will lower ATM fees significantly reduce the number of high-value customers who leave?\"\n",
    "\n",
    "#### business success criteria \n",
    "\n",
    "Describe the criteria for a successful or useful outcome to the project from the business point of view. This might be quite specific and able to be measured objectively, such as reduction of customer churn to a certain level or general and subjective such as \"give useful insights into the relationships.\" In the latter case it should be indicated who makes the subjective judgment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess situation\n",
    "-------------\n",
    "\n",
    "### task\n",
    "\n",
    "This task involves more detailed fact-finding about all of the resources,constraints, assumptions and other factors that should be considered in determining the data analysis goal and project plan. In the previous task, your objective is to quickly get to the crux of the situation. Here, you want to flesh out the details.\n",
    "\n",
    "### output\n",
    "\n",
    "#### inventory of resources\n",
    "\n",
    "List the resources available to the project, including: personnel (business experts, data experts, technical support, data mining personnel), data (fixed extracts, access to live warehoused or operational data), computing resources (hardware platforms) and software (data miningtools, other relevant software).\n",
    "\n",
    "#### requirements, assumptions and constraints\n",
    "\n",
    "List all requirements of the project including schedule of completion, comprehensibility and quality of results and security as well as legal issues.As part of this output, make sure that you are allowed to use the data. List the assumptions made by the project. \n",
    "\n",
    "These may be assumptions about the data that can be checked during data mining, but may also include non-checkable assumptions about the business upon which the project rests. It is particularly important to list the latter if they form conditions on the validity of the results.\n",
    "\n",
    "List the constraints on the project. These may be constraints on the availability of resources, but may also include technological constraints such as the size of data that it is practical to use for modeling.\n",
    "\n",
    "#### Risks and contingencies \n",
    "\n",
    "List the risks or events that might occur to delay the project or cause it to fail. List the corresponding contingency plans; what action will be taken if the risks happen.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "Compile a glossary of terminology relevant to the project. This may include two components: \n",
    "(1) A glossary of relevant business terminology, which forms part of the business understanding available to the project. Constructing this glossary is a useful \"knowledge elicitation\" and education exercise.\n",
    "(2) A glossary of data mining terminology, illustrated with examples relevant to the business problem in question.\n",
    "\n",
    "#### Costs and benefits\n",
    "\n",
    "Construct a cost-benefit analysis for the project, which compares the costs of the project with the potential benefit to the business if it is successful. The comparison should be as specific as possible, for example using monetary measures in a commercial situation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine data mining goals\n",
    "-----------\n",
    "\n",
    "### task\n",
    "\n",
    "A business goal states objectives in business terminology. A data mining goal states project objectives in technical terms. For example, the business goal might be \"Increase catalog sales to existing customers.\" A data mining goal might be \"Predict how many widgets a customer will buy, given their purchases over the past three years, demographic information (age, salary, city, etc.) and the price of the item.\"\n",
    "\n",
    "### output\n",
    "\n",
    "#### data mining goals\n",
    "\n",
    "Describe the intended outputs of the project that enables the achievement of the business objectives.\n",
    "\n",
    "#### data mining success criteria\n",
    "\n",
    "Define the criteria for a successful outcome to the project in technical terms, for example a certain level of predictive accuracy or a propensity to purchase profile with a given degree of \"lift.\" As with business success criteria, it may be necessary to describe these in subjective terms, in which case the person or persons making the subjective judgment should be identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce project plan\n",
    "----------------\n",
    "\n",
    "### task\n",
    "\n",
    "Describe the intended plan for achieving the data mining goals and thereby achieving the business goals. The plan should specify the anticipated set of steps to be performed during the rest of the project including an initial selection of tools and techniques.\n",
    "\n",
    "### output\n",
    "\n",
    "#### project plan\n",
    "\n",
    "List the stages to be executed in the project, together with duration, resources required, inputs, outputs and dependencies. Where possible make explicit the large-scale iterations in the data mining process, for example repetitions of the modeling and evaluation phases. As part of the project plan, it is also important to analyze dependencies between time schedule and risks. Mark results of these analyses explicitly in the project plan, ideally with actions and recommendations if the risks appear.\n",
    "\n",
    "Note: the project plan contains detailed plans for each phase. For example, decide at this point which evaluation strategy will be used in the evaluation phase. The project plan is a dynamic document in the sense that at the end of each phase a review of progress and achievements is necessary and an update of the project plan accordingly is recommended. Specific review points for these reviews are part of the project plan, too.\n",
    "\n",
    "#### initial assessment of tools and techniques\n",
    "\n",
    "At the end of the first phase, the project also performs an initial assessment of tools and techniques. Here, you select a data mining tool that supports various methods for different stages of the process, for example. It is important to assess tools and techniques early in the process since the selection of tools and techniques possibly influences the entire project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding\n",
    "------------\n",
    "The data understanding phase starts with an initial data collection and proceeds withactivities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data or to detect interesting subsets to form hypothesesfor hidden information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect initial data\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Acquire within the project the data (or access to the data) listed in the\n",
    "project resources. This initial collection includes data loading if necessary\n",
    "for data understanding. For example, if you apply a specific tool for data\n",
    "understanding, it makes perfect sense to load your data into this tool.\n",
    "This effort possibly leads to initial data preparation steps.\n",
    "\n",
    "Note: if you acquire multiple data sources, integration is an additional\n",
    "issue, either here or in the later data preparation phase.\n",
    "\n",
    "### output\n",
    "\n",
    "List the dataset (or datasets) acquired, together with their locations\n",
    "within the project, the methods used to acquire them and any problems\n",
    "encountered. Record problems encountered and any solutions achieved\n",
    "to aid with future replication of this project or with the execution of\n",
    "similar future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv(path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel file\n",
    "df = pd.read_excel(path + file_name, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe data\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Examine the “gross” or “surface” properties of the acquired data and\n",
    "report on the results.\n",
    "\n",
    "### output\n",
    "\n",
    "Describe the data which has been acquired, including: the format of\n",
    "the data, the quantity of data, for example number of records and fields\n",
    "in each table, the identities of the fields and any other surface features\n",
    "of the data which have been discovered. Does the data acquired satisfy\n",
    "the relevant requirements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of records and fields\n",
    "df.shape # (records, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail\n",
    "df.tail(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "This task tackles the data mining questions, which can be addressed\n",
    "using querying, visualization and reporting. These include: distribution\n",
    "of key attributes, for example the target attribute of a prediction task;\n",
    "relations between pairs or small numbers of attributes; results of\n",
    "simple aggregations; properties of significant sub-populations; simple\n",
    "statistical analyses. These analyses may address directly the data mining goals; they may also contribute to or refine the data description\n",
    "and quality reports and feed into the transformation and other data\n",
    "preparation needed for further analysis.\n",
    "\n",
    "### output\n",
    "\n",
    "Describe results of this task including first findings or initial hypothesis and their impact on the remainder of the project. If appropriate,\n",
    "include graphs and plots, which indicate data characteristics or lead\n",
    "to interesting data subsets for further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type\n",
    "df.shipping_date = pd.to_datetime(df.shipping_date, format='%Y/%m/%d')\n",
    "df.price = df.price.astype('int64')\n",
    "df.quantity = df.quantity.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "sns.distplot(df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "sns.distplot(df.quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data wrangling\n",
    "data = df.groupby('shipping_date', as_index=False).agg({'price': np.sum, 'quantity': np.sum})\n",
    "sns.lineplot(data=data, x=\"shipping_date\", y=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas profiling\n",
    "profile = ProfileReport(df, minimal=True)\n",
    "profile.to_file(path + \"data_understanding_report.html\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify data quality\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Examine the quality of the data, addressing questions such as: is the\n",
    "data complete (does it cover all the cases required)? Is it correct or\n",
    "does it contain errors and if there are errors how common are they?\n",
    "Are there missing values in the data? If so how are they represented,\n",
    "where do they occur and how common are they?\n",
    "\n",
    "### output\n",
    "\n",
    "List the results of the data quality verification; if quality problems\n",
    "exist, list possible solutions. Solutions to data quality problems\n",
    "generally depend heavily on both data and business knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate row\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outlier\n",
    "from scipy import stats\n",
    "z_thr = 3.0\n",
    "df[(np.abs(stats.zscore(df.select_dtypes(include=int))) > z_thr).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of unique value\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique value\n",
    "for feature in df.columns:\n",
    "  print('------' + feature + '------')\n",
    "  print(np.sort(df[feature].unique()))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "---------------------\n",
    "The data preparation phase covers all activities to construct the final dataset (data that will be fed into the modeling tool(s)) from the initial raw data. Data preparation tasks are likely to be performed multiple times and not in any prescribed order. Tasks include table, record and attribute selection as well as transformation and cleaning of data for modeling tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Decide on the data to be used for analysis. Criteria include relevance to the data mining goals, quality and technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.\n",
    "\n",
    "### output\n",
    "\n",
    "List the data to be included/excluded and the reasons for these decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select data source\n",
    "\n",
    "| # | data | included/excluded | reasons | quality | volume/data types |\n",
    "|:---:|:---|:---|:---|:---|:---|\n",
    "| 1 |  | included |  |  |  |\n",
    "\n",
    "#### select attributes & records\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "------------------\n",
    "\n",
    "### task\n",
    "\n",
    "Raise the data quality to the level required by the selected analysistechniques. This may involve selection of clean subsets of the data, the insertion of suitable defaults or more ambitious techniques such as the estimation of missing data by modeling.\n",
    "\n",
    "### output\n",
    "\n",
    "Describe what decisions and actions were taken to address the data quality problems reported during the verify data quality task of the data understanding phase. Transformations of the data for cleaning purposes and the possible impact on the analysis results should be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnncesary columns\n",
    "columns = [] # remove no columns if array is empty\n",
    "df = df.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean outlier\n",
    "\n",
    "## clean minus data\n",
    "column = ''\n",
    "del_flg = df[column] < 0\n",
    "df = df.drop(df[del_flg].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean duplicate records\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "This task includes constructive data preparation operations such as the production of derived attributes, entire new records or transformed values for existing attributes.\n",
    "\n",
    "### output\n",
    "\n",
    "#### derived attributes\n",
    "\n",
    "Derived attributes are new attributes that are constructed from one or more existing attributes in the same record. \n",
    "\n",
    "Examples: area = length * width\n",
    "\n",
    "#### generated records\n",
    "\n",
    "Describe the creation of completely new records. \n",
    "\n",
    "Example: create records for customers who made no purchase during the past year.There was no reason to have such records in the raw data, but for modeling purposes it might make sense to explicitly represent the fact that certain customers made zero purchases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate data\n",
    "-------------\n",
    "\n",
    "### task\n",
    "\n",
    "These are methods whereby information is combined from multiple tables or records to create new records or values.\n",
    "\n",
    "### output\n",
    "\n",
    "Merging tables refers to joining together two or more tables that have different information about the same objects. \n",
    "\n",
    "Example: a retail chainhas one table with information about each store's general characteristics(e.g., floor space, type of mall), another table with summarized sales data (e.g., profit, percent change in sales from previous year) and another with information about the demographics of the surrounding area. Each of these tables contains one record for each store. These tables can be merged together into a new table with one record foreach store, combining fields from the source tables.\n",
    "\n",
    "Merged data also covers aggregations. Aggregation refers to operations where new values are computed by summarizing together information from multiple records and/or tables. For example, converting a table ofcustomer purchases where there is one record for each purchase into a new table where there is one record for each customer, with fields such as number of purchases, average purchase amount, percent of orders charged to credit card, percent of items under promotion, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format data\n",
    "----------------\n",
    "\n",
    "### task\n",
    "\n",
    "Formatting transformations refer to primarily syntactic modifications made to the data that do not change its meaning, but might be required by the modeling tool.\n",
    "\n",
    "### output\n",
    "\n",
    "Some tools have requirements on the order of the attributes, such as the first field being a unique identifier for each record or the last field being the outcome field the model is to predict.\n",
    "\n",
    "It might be important to change the order of the records in the dataset. Perhaps the modeling tool requires that the records be sorted according to the value of the outcome attribute. A common situation is that the records of the dataset are initially ordered in some way but the modeling algorithm needs them to be in a fairly random order. For example, when using neural networks it is generally best for the records to be presented in a random order although some tools handle this automatically with-out explicit user intervention.\n",
    "\n",
    "Additionally, there are purely syntactic changes made to satisfy the requirements of the specific modeling tool. \n",
    "\n",
    "Examples: removing commas from within text fields in comma-delimited data files, trimming all values to a maximum of 32 characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "---------------------\n",
    "In this phase, various modeling techniques are selected and applied and their parameters are calibrated to optimal values. Typically, there are several techniques for the same data mining problem type. Some techniques have specific requirements on the form of data. Therefore, stepping back to the data preparation phase is often necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select modeling technique\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "As the first step in modeling, select the actual modeling technique that is to be used. Whereas you possibly already selected a tool in business understanding, this task refers to the specific modeling technique, e.g.,decision tree building with C4.5 or neural network generation with back propagation. If multiple techniques are applied, perform this task for each technique separately.\n",
    "\n",
    "### output\n",
    "\n",
    "#### modeling technique\n",
    "\n",
    "Document the actual modeling technique that is to be used.\n",
    "\n",
    "#### modeling assumptions\n",
    "\n",
    "Many modeling techniques make specific assumptions on the data, e.g.,all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any such assumptions made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test design\n",
    "------------------\n",
    "\n",
    "### task\n",
    "\n",
    "Before we actually build a model, we need to generate a procedure or mechanism to test the model's quality and validity. For example, in supervised data mining tasks such as classification, it is common to use error rates as quality measures for data mining models. Therefore, we typically separate the dataset into train and test set, build the model on the train set and estimate its quality on the separate test set.\n",
    "\n",
    "### output\n",
    "\n",
    "Describe the intended plan for training, testing and evaluating the models. A primary component of the plan is to decide how to divide the available dataset into training data, test data and validation datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Run the modeling tool on the prepared dataset to create one or more models.\n",
    "\n",
    "### output\n",
    "\n",
    "#### parameter settings \n",
    "\n",
    "With any modeling tool, there are often a large number of parameters that can be adjusted. List the parameters and their chosen value, along with the rationale for the choice of parameter settings. \n",
    "\n",
    "#### models \n",
    "\n",
    "These are the actual models produced by the modeling tool, not a report.\n",
    "\n",
    "#### model description\n",
    "\n",
    "describe the resultant model. Report on the interpretation of the models and document any difficulties encountered with their meanings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "## closed test\n",
    "X_train, X_test, y_train, y_test = X, X, y, y\n",
    "\n",
    "## random split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', random_state=None)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "!pip install optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "params = {\n",
    "    \"objective\" : \"multiclass\",\n",
    "    \"metric\" : \"multi_logloss\",\n",
    "    \"num_class\" : len(y.unique())\n",
    "}\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval)\n",
    "y_prob = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = np.argmax(y_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess model\n",
    "-------------\n",
    "\n",
    "### task\n",
    "\n",
    "The data mining engineer interprets the models according to his domain knowledge, the data mining success criteria and the desired test design. This task interferes with the subsequent evaluation phase. Whereas the data mining engineer judges the success of the application of modeling and discovery techniques more technically, he contacts business analysts and domain experts later in order to discuss the data mining results in the business context. Moreover, this task only considers models whereas the evaluation phase also takes into account all other results that were produced in the course of the project. The data mining engineer tries to rank the models. He assesses the models according to the evaluation criteria. As far as possible he also takes into account business objectives and business success criteria. In most data mining projects, the data mining engineer applies a single technique more than once or generates data mining results with different alternative techniques. In this task, he also compares all results according to the evaluation criteria.\n",
    "\n",
    "### output\n",
    "\n",
    "#### model assessment\n",
    "\n",
    "Summarize results of this task, list qualities of generated models (e.g.,in terms of accuracy) and rank their quality in relation to each other. \n",
    "\n",
    "#### revised parameter settings\n",
    "\n",
    "According to the model assessment, revise parameter settings and tune them for the next run in the Build Model task. Iterate model building and assessment until you strongly believe that you found the best model(s). Document all such revisions and assessments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(str('{:.1g}'.format(accuracy * 100)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "---------------------\n",
    "At this stage in the project you have built a model (or models) that appears to have high quality from a data analysis perspective. Before proceeding to final deployment of the model, it is important to more thoroughly evaluate the model and review the steps executed to construct the model to be certain it properly achieves the business objectives. A key objective is to determine if there is some important business issue that has not been sufficiently considered. At the end of this phase, a decision on the use of the data mining results should be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "---------------------\n",
    "\n",
    "Creation of the model is generally not the end of the project. Even if the purpose of the model is to increase knowledge of the data, the knowledge gained will need to be organized and presented in a way that the customer can use it. It often involves applying \"live\" models within an organization's decision making processes, for example in real-time personalization of Web pages or repeated scoring of marketing databases. However, depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data mining process across the enterprise. In many cases it is the customer, not the data analyst, who carries out the deployment steps. However, even if the analyst will not carry out the deployment effort it is important for the customer to understand up front what actions need to be carried out in order to actually make use of the created models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan deployment\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "In order to deploy the data mining result(s) into the business, this task takes the evaluation results and concludes a strategy for deployment. If a general procedure has been identified to create the relevant model(s), this procedure is documented here for later deployment.\n",
    "\n",
    "### output\n",
    "\n",
    "Summarize deployment strategy including necessary steps and how to perform them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan monitoring & maintenance\n",
    "------------------\n",
    "\n",
    "### task\n",
    "\n",
    "Monitoring and maintenance are important issues if the data mining result becomes part of the day-to-day business and its environment. A careful preparation of a maintenance strategy helps to avoid unnecessarily long periods of incorrect usage of data mining results. In order to monitor the deployment of the data mining result(s), the project needs a detailed plan on the monitoring process. This plan takes into account the specific type of deployment.\n",
    "\n",
    "### output\n",
    "\n",
    "Summarize monitoring and maintenance strategy including necessary steps and how to perform them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce final report\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "At the end of the project, the project leader and his team write up a final report. Depending on the deployment plan, this report may be only a summary of the project and its experiences (if they have not already been documented as an ongoing activity) or it may be a final and comprehensive presentation of the data mining result(s)\n",
    "\n",
    "### output\n",
    "\n",
    "#### final report \n",
    "\n",
    "This is the final written report of the data mining engagement. It includes all of the previous deliverables and summarizes and organizes the results.\n",
    "\n",
    "#### final presentation \n",
    "\n",
    "There will also often be a meeting at the conclusion of the project where the results are verbally presented to the customer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review project\n",
    "-------------\n",
    "\n",
    "## task\n",
    "\n",
    "Assess what went right and what went wrong, what was done well and what needs to be improved.\n",
    "\n",
    "## output\n",
    "\n",
    "Summarize important experiences made during the project. For example, pitfalls, misleading approaches or hints for selecting the best suited data mining techniques in similar situations could be part of this documentation. In ideal projects, experience documentation covers also any reports that have been written by individual project members during the project phases and their tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate result\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "Previous evaluation steps dealt with factors such as the accuracy and generality of the model. This step assesses the degree to which the model meets the business objectives and seeks to determine if there is some business reason why this model is deficient. Another option of evaluation is to test the model(s) on test applications in the real application if time and budget constraints permit.\n",
    "\n",
    "Moreover, evaluation also assesses other data mining results generated. Data mining results cover models which are necessarily related to the original business objectives and all other findings which are not necessarily related to the original business objectives but might also unveil additional challenges, information or hints for future directions.\n",
    "\n",
    "### output\n",
    "\n",
    "#### assessment of data mining results with respect to business success criteria \n",
    "\n",
    "Summarize assessment results in terms of business success criteria including a final statement whether the project already meets the initial business objectives.\n",
    "\n",
    "#### approved models \n",
    "\n",
    "After model assessment with respect to business success criteria, the gen-erated models that meet the selected criteria become approved models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review process\n",
    "------------------\n",
    "\n",
    "### task\n",
    "\n",
    "At this point the resultant model hopefully appears to be satisfactory and to satisfy business needs. It is now appropriate to do a more thorough review of the data mining engagement in order to determine if there is any important factor or task that has somehow been overlooked. This review also covers quality assurance issues, e.g., did we correctly build the model? Did we only use attributes that we are allowed to use and that are available for future analyses?\n",
    "\n",
    "### output\n",
    "\n",
    "Summarize the process review and highlight activities that have been missed and/or should be repeated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine next steps\n",
    "----------\n",
    "\n",
    "### task\n",
    "\n",
    "According to the assessment results and the process review, the project decides how to proceed at this stage. The project needs to decide whether to finish this project and move on to deployment if appropriate or whether to initiate further iterations or set up new data mining projects. This task includes analyses of remaining resources and budget that influences the decisions.\n",
    "\n",
    "### output\n",
    "\n",
    "#### list of possible actions \n",
    "\n",
    "List the potential further actions along with the reasons for and against each option.\n",
    "\n",
    "#### Decision \n",
    "\n",
    "Describe the decision as to how to proceed along with the rationale.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
